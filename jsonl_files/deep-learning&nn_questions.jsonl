{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of the activation function in a neural network?", "options": {"A": "To initialize the weights", "B": "To add non-linearity to the model", "C": "To normalize the data", "D": "To reduce overfitting"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following activation functions is commonly used in neural networks?", "options": {"A": "Sigmoid", "B": "ReLU", "C": "Tanh", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is a neuron in the context of neural networks?", "options": {"A": "A unit of computation that applies a linear transformation followed by an activation function", "B": "A data storage unit", "C": "A loss function", "D": "None of the above"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following is a common loss function used in neural networks?", "options": {"A": "Mean Squared Error", "B": "Cross-Entropy Loss", "C": "Hinge Loss", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the primary purpose of a convolutional layer in a CNN?", "options": {"A": "To perform linear regression", "B": "To detect spatial features in input data", "C": "To normalize data", "D": "To reduce the dimensionality of data"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following techniques is used to prevent overfitting in neural networks?", "options": {"A": "Dropout", "B": "Batch Normalization", "C": "Early Stopping", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is backpropagation?", "options": {"A": "A method to initialize the weights", "B": "A technique to compute the gradient of the loss function with respect to each weight", "C": "A way to normalize the input data", "D": "A regularization technique"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following is true about the ReLU activation function?", "options": {"A": "It can introduce sparsity in the activations", "B": "It is computationally efficient", "C": "It can suffer from the dying ReLU problem", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the vanishing gradient problem?", "options": {"A": "When the gradients become very small, making it difficult for the model to learn", "B": "When the gradients become very large, causing the model to diverge", "C": "When the model overfits the training data", "D": "None of the above"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of the softmax function?", "options": {"A": "To normalize the output to a probability distribution", "B": "To add non-linearity to the model", "C": "To reduce overfitting", "D": "To initialize the weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of the pooling layer in a CNN?", "options": {"A": "To reduce the dimensionality of the input", "B": "To increase the computational complexity", "C": "To normalize the input data", "D": "To add non-linearity to the model"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of batch normalization?", "options": {"A": "To prevent overfitting", "B": "To normalize the input features of each batch", "C": "To increase the learning rate", "D": "To initialize the weights"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of the LSTM layer in a neural network?", "options": {"A": "To capture long-term dependencies in sequence data", "B": "To reduce the dimensionality of the input data", "C": "To normalize the input data", "D": "To add non-linearity to the model"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is true about the dropout technique?", "options": {"A": "It reduces overfitting by randomly setting a fraction of the activations to zero during training", "B": "It reduces the dimensionality of the input data", "C": "It increases the learning rate", "D": "It normalizes the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of the cross-entropy loss function?", "options": {"A": "To measure the performance of a classification model", "B": "To measure the performance of a regression model", "C": "To initialize the weights", "D": "To normalize the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is a characteristic of the tanh activation function?", "options": {"A": "It outputs values between 0 and 1", "B": "It is computationally expensive", "C": "It outputs values between -1 and 1", "D": "It suffers from the vanishing gradient problem"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the primary purpose of using an embedding layer in a neural network?", "options": {"A": "To reduce the dimensionality of the input data", "B": "To transform categorical data into a continuous vector space", "C": "To add non-linearity to the model", "D": "To normalize the input data"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is true about the gradient clipping technique?", "options": {"A": "It reduces overfitting by adding a penalty to the loss function", "B": "It normalizes the input data", "C": "It prevents the gradients from becoming too large", "D": "It increases the learning rate"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of using a recurrent neural network (RNN)?", "options": {"A": "To process fixed-length input data", "B": "To process sequential data", "C": "To reduce the dimensionality of the input data", "D": "To normalize the input data"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is true about the Adam optimizer?", "options": {"A": "It combines the advantages of both AdaGrad and RMSProp", "B": "It requires less memory", "C": "It performs well with sparse gradients", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the batch normalization technique?", "options": {"A": "It increases the learning rate", "B": "It reduces overfitting", "C": "It accelerates the training process by reducing internal covariate shift", "D": "It normalizes the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the attention mechanism in neural networks?", "options": {"A": "To increase the learning rate", "B": "To reduce the dimensionality of the input data", "C": "To focus on relevant parts of the input sequence", "D": "To normalize the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the Transformer architecture in neural networks?", "options": {"A": "It can handle sequential data efficiently", "B": "It reduces the training time", "C": "It captures long-range dependencies in the data", "D": "Both A and C"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "Which of the following is true about the gated recurrent unit (GRU)?", "options": {"A": "It is a type of RNN that can capture long-term dependencies", "B": "It has fewer parameters compared to LSTM", "C": "It does not suffer from the vanishing gradient problem", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the ReLU activation function?", "options": {"A": "It adds non-linearity to the model", "B": "It avoids the vanishing gradient problem", "C": "It is computationally efficient", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the max-pooling layer in a CNN?", "options": {"A": "To increase the dimensionality of the input data", "B": "To reduce the dimensionality of the input data", "C": "To add non-linearity to the model", "D": "To normalize the input data"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of using the softmax function in the output layer of a neural network?", "options": {"A": "To normalize the output to a probability distribution", "B": "To reduce overfitting", "C": "To add non-linearity to the model", "D": "To initialize the weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the dropout technique in neural networks?", "options": {"A": "It reduces overfitting by randomly setting a fraction of the activations to zero during training", "B": "It reduces the dimensionality of the input data", "C": "It increases the learning rate", "D": "It normalizes the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the vanishing gradient problem?", "options": {"A": "When the gradients become very small, making it difficult for the model to learn", "B": "When the gradients become very large, causing the model to diverge", "C": "When the model overfits the training data", "D": "None of the above"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the Adam optimizer?", "options": {"A": "It requires less memory", "B": "It performs well with sparse gradients", "C": "It combines the advantages of both AdaGrad and RMSProp", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the Transformer architecture in neural networks?", "options": {"A": "It can handle sequential data efficiently", "B": "It reduces the training time", "C": "It captures long-range dependencies in the data", "D": "Both A and C"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the attention mechanism in neural networks?", "options": {"A": "To increase the learning rate", "B": "To reduce the dimensionality of the input data", "C": "To focus on relevant parts of the input sequence", "D": "To normalize the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the batch normalization technique?", "options": {"A": "To increase the learning rate", "B": "To reduce overfitting", "C": "To accelerate the training process by reducing internal covariate shift", "D": "It normalizes the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "Which of the following is true about the gated recurrent unit (GRU)?", "options": {"A": "It is a type of RNN that can capture long-term dependencies", "B": "It has fewer parameters compared to LSTM", "C": "It does not suffer from the vanishing gradient problem", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of using the softmax function in the output layer of a neural network?", "options": {"A": "To normalize the output to a probability distribution", "B": "To reduce overfitting", "C": "To add non-linearity to the model", "D": "To initialize the weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the vanishing gradient problem?", "options": {"A": "When the gradients become very small, making it difficult for the model to learn", "B": "When the gradients become very large, causing the model to diverge", "C": "When the model overfits the training data", "D": "None of the above"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the Adam optimizer?", "options": {"A": "It requires less memory", "B": "It performs well with sparse gradients", "C": "It combines the advantages of both AdaGrad and RMSProp", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the ReLU activation function?", "options": {"A": "It adds non-linearity to the model", "B": "It avoids the vanishing gradient problem", "C": "It is computationally efficient", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the dropout technique in neural networks?", "options": {"A": "It reduces overfitting by randomly setting a fraction of the activations to zero during training", "B": "It reduces the dimensionality of the input data", "C": "It increases the learning rate", "D": "It normalizes the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the max-pooling layer in a CNN?", "options": {"A": "To increase the dimensionality of the input data", "B": "To reduce the dimensionality of the input data", "C": "To add non-linearity to the model", "D": "To normalize the input data"}, "correct_answers": ["B"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the Transformer architecture in neural networks?", "options": {"A": "It can handle sequential data efficiently", "B": "It reduces the training time", "C": "It captures long-range dependencies in the data", "D": "Both A and C"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the gated recurrent unit (GRU)?", "options": {"A": "It is a type of RNN that can capture long-term dependencies", "B": "It has fewer parameters compared to LSTM", "C": "It does not suffer from the vanishing gradient problem", "D": "All of the above"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the batch normalization technique?", "options": {"A": "It increases the learning rate", "B": "It reduces overfitting", "C": "It accelerates the training process by reducing internal covariate shift", "D": "It normalizes the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the main advantage of using the attention mechanism in neural networks?", "options": {"A": "It increases the learning rate", "B": "It reduces the dimensionality of the input data", "C": "It focuses on relevant parts of the input sequence", "D": "It normalizes the input data"}, "correct_answers": ["C"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the role of a perceptron in a neural network?", "options": {"A": "To aggregate input features linearly", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following best describes the perceptron learning rule?", "options": {"A": "Adjust weights based on the difference between the predicted and actual output", "B": "Increase weights for every input", "C": "Decrease weights for every input", "D": "Keep weights constant"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the output of the perceptron when the input is [2, 3]?", "options": {"A": "1", "B": "0", "C": "-1", "D": "2"}, "correct_answers": ["A"], "is_multiple_choice": false, "code": "```python\nimport numpy as np\nclass Perceptron:\n    def __init__(self, input_size, lr=1, epochs=100):\n        self.W = np.zeros(input_size + 1)\n        self.lr = lr\n        self.epochs = epochs\n\n    def activation_fn(self, x):\n        return 1 if x >= 0 else 0\n\n    def predict(self, x):\n        z = self.W.T.dot(np.insert(x, 0, 1))\n        return self.activation_fn(z)\n\n    def fit(self, X, d):\n        for _ in range(self.epochs):\n            for i in range(d.shape[0]):\n                x = np.insert(X[i], 0, 1)\n                y = self.activation_fn(self.W.T.dot(x))\n                self.W = self.W + self.lr * (d[i] - y) * x\n\nX = np.array([[2, 3], [1, 1], [2, 1], [1, 2]])\nd = np.array([1, 0, 1, 0])\nperceptron = Perceptron(input_size=2)\nperceptron.fit(X, d)\nprint(perceptron.W)\n```","is_multiple_choice": false}}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which activation function would you use to introduce non-linearity in a neural network?", "options": {"A": "ReLU", "B": "Linear", "C": "Identity", "D": "Binary Step"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "How do you implement the ReLU activation function in Python?", "options": {"A": "def relu(x): return np.maximum(0, x)", "B": "def relu(x): return np.minimum(0, x)", "C": "def relu(x): return np.abs(x)", "D": "def relu(x): return x**2"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the derivative of the sigmoid activation function used for backpropagation?", "options": {"A": "sigmoid(x) * (1 - sigmoid(x))", "B": "1 - sigmoid(x)", "C": "sigmoid(x)", "D": "x * (1 - x)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of a loss function in neural networks?", "options": {"A": "To measure the difference between the predicted and actual output", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which loss function is commonly used for binary classification?", "options": {"A": "Binary Cross-Entropy", "B": "Mean Squared Error", "C": "Hinge Loss", "D": "Categorical Cross-Entropy"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement the binary cross-entropy loss function in Python?", "options": {"A": "def binary_cross_entropy(y_true, y_pred): epsilon = 1e-15 y_pred = np.clip(y_pred, epsilon, 1 - epsilon) return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))", "B": "def binary_cross_entropy(y_true, y_pred): return np.mean(np.square(y_true - y_pred))", "C": "def binary_cross_entropy(y_true, y_pred): return np.mean(np.abs(y_true - y_pred))", "D": "def binary_cross_entropy(y_true, y_pred): return np.sum(np.square(y_true - y_pred))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of backpropagation in neural networks?", "options": {"A": "To update the weights of the network by propagating the error gradient backward", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How is the gradient of the loss with respect to weights calculated in backpropagation?", "options": {"A": "By applying the chain rule of calculus", "B": "By averaging the weights", "C": "By summing the weights", "D": "By multiplying the weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How do you implement the gradient descent algorithm in Python?", "options": {"A": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): predictions = np.dot(X, weights) error = predictions - y gradients = np.dot(X.T, error) / m weights -= learning_rate * gradients return weights", "B": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): gradients = np.dot(X.T, y) / m weights += learning_rate * gradients return weights", "C": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): error = np.dot(X, weights) gradients = np.dot(X.T, error) / m weights *= learning_rate * gradients return weights", "D": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): error = np.dot(X, weights) gradients = np.dot(X.T, error) / m weights -= learning_rate * gradients return weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the main purpose of regularization in neural networks?", "options": {"A": "To prevent overfitting", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which regularization technique involves adding a penalty proportional to the absolute value of the weights?", "options": {"A": "L1 Regularization", "B": "L2 Regularization", "C": "Dropout", "D": "Early Stopping"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement L2 regularization in the loss function?", "options": {"A": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.sum(np.square(weights))", "B": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.sum(np.abs(weights))", "C": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.mean(np.square(weights))", "D": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.mean(np.abs(weights))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of an activation function in a neural network?", "options": {"A": "To introduce non-linearity into the network", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following is a commonly used activation function in neural networks?", "options": {"A": "ReLU", "B": "Linear", "C": "Exponential", "D": "Step"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How do you implement the sigmoid activation function in Python?", "options": {"A": "def sigmoid(x): return 1 / (1 + np.exp(-x))", "B": "def sigmoid(x): return np.exp(x) / (1 + np.exp(x))", "C": "def sigmoid(x): return x / (1 + np.abs(x))", "D": "def sigmoid(x): return np.tanh(x)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the vanishing gradient problem?", "options": {"A": "A problem where gradients become too small for effective learning in deep networks", "B": "A problem where gradients become too large for effective learning in deep networks", "C": "A problem where gradients oscillate and do not converge", "D": "A problem where gradients diverge to infinity"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement the mean squared error (MSE) loss function in Python?", "options": {"A": "def mse(y_true, y_pred): return np.mean((y_true - y_pred)**2)", "B": "def mse(y_true, y_pred): return np.sum((y_true - y_pred)**2)", "C": "def mse(y_true, y_pred): return np.abs(y_true - y_pred)", "D": "def mse(y_true, y_pred): return np.mean(np.abs(y_true - y_pred))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the main purpose of using the ReLU activation function?", "options": {"A": "To introduce non-linearity into the model", "B": "To linearly scale inputs", "C": "To reduce dimensionality", "D": "To sort data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How does the learning rate affect the training of a neural network?", "options": {"A": "It determines the step size for updating weights", "B": "It sorts the data", "C": "It compresses the data", "D": "It encrypts the data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is a common issue when the learning rate is set too high?", "options": {"A": "The model may diverge and fail to converge", "B": "The model may learn too quickly", "C": "The model may encrypt data", "D": "The model may compress data inefficiently"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the role of a loss function in training neural networks?", "options": {"A": "To measure how well the model's predictions match the actual outcomes", "B": "To sort the training data", "C": "To compress the input data", "D": "To encrypt the model weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is an example of a non-linear activation function?", "options": {"A": "ReLU", "B": "Linear", "C": "Identity", "D": "Binary Step"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the softmax function in neural networks?", "options": {"A": "To convert logits to probabilities for multi-class classification", "B": "To sort data into different classes", "C": "To compress output vectors", "D": "To encrypt class labels"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement the softmax function in Python?", "options": {"A": "def softmax(x): exps = np.exp(x - np.max(x)) return exps / np.sum(exps, axis=0)", "B": "def softmax(x): return np.log(x)", "C": "def softmax(x): return np.tanh(x)", "D": "def softmax(x): return x / np.sum(x)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the primary difference between batch gradient descent and stochastic gradient descent (SGD)?", "options": {"A": "Batch gradient descent uses the entire dataset to compute gradients, while SGD uses one sample at a time", "B": "Batch gradient descent sorts data, while SGD encrypts data", "C": "Batch gradient descent compresses data, while SGD scales data", "D": "Batch gradient descent is faster than SGD"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of dropout in training neural networks?", "options": {"A": "To prevent overfitting by randomly dropping units during training", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How do you implement the ReLU activation function in Python?", "options": {"A": "def relu(x): return np.maximum(0, x)", "B": "def relu(x): return np.minimum(0, x)", "C": "def relu(x): return np.abs(x)", "D": "def relu(x): return x**2"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the derivative of the sigmoid activation function used for backpropagation?", "options": {"A": "sigmoid(x) * (1 - sigmoid(x))", "B": "1 - sigmoid(x)", "C": "sigmoid(x)", "D": "x * (1 - x)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of a loss function in neural networks?", "options": {"A": "To measure the difference between the predicted and actual output", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which loss function is commonly used for binary classification?", "options": {"A": "Binary Cross-Entropy", "B": "Mean Squared Error", "C": "Hinge Loss", "D": "Categorical Cross-Entropy"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement the binary cross-entropy loss function in Python?", "options": {"A": "def binary_cross_entropy(y_true, y_pred): epsilon = 1e-15 y_pred = np.clip(y_pred, epsilon, 1 - epsilon) return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))", "B": "def binary_cross_entropy(y_true, y_pred): return np.mean(np.square(y_true - y_pred))", "C": "def binary_cross_entropy(y_true, y_pred): return np.mean(np.abs(y_true - y_pred))", "D": "def binary_cross_entropy(y_true, y_pred): return np.sum(np.square(y_true - y_pred))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of backpropagation in neural networks?", "options": {"A": "To update the weights of the network by propagating the error gradient backward", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How is the gradient of the loss with respect to weights calculated in backpropagation?", "options": {"A": "By applying the chain rule of calculus", "B": "By averaging the weights", "C": "By summing the weights", "D": "By multiplying the weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How do you implement the gradient descent algorithm in Python?", "options": {"A": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): predictions = np.dot(X, weights) error = predictions - y gradients = np.dot(X.T, error) / m weights -= learning_rate * gradients return weights", "B": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): gradients = np.dot(X.T, y) / m weights += learning_rate * gradients return weights", "C": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): error = np.dot(X, weights) gradients = np.dot(X.T, error) / m weights *= learning_rate * gradients return weights", "D": "def gradient_descent(X, y, weights, learning_rate, iterations): m = len(y) for i in range(iterations): error = np.dot(X, weights) gradients = np.dot(X.T, error) / m weights -= learning_rate * gradients return weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the main purpose of regularization in neural networks?", "options": {"A": "To prevent overfitting", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which regularization technique involves adding a penalty proportional to the absolute value of the weights?", "options": {"A": "L1 Regularization", "B": "L2 Regularization", "C": "Dropout", "D": "Early Stopping"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement L2 regularization in the loss function?", "options": {"A": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.sum(np.square(weights))", "B": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.sum(np.abs(weights))", "C": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.mean(np.square(weights))", "D": "def l2_regularization_loss(weights, lambda_): return lambda_ * np.mean(np.abs(weights))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of an activation function in a neural network?", "options": {"A": "To introduce non-linearity into the network", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "Which of the following is a commonly used activation function in neural networks?", "options": {"A": "ReLU", "B": "Linear", "C": "Exponential", "D": "Step"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How do you implement the sigmoid activation function in Python?", "options": {"A": "def sigmoid(x): return 1 / (1 + np.exp(-x))", "B": "def sigmoid(x): return np.exp(x) / (1 + np.exp(x))", "C": "def sigmoid(x): return x / (1 + np.abs(x))", "D": "def sigmoid(x): return np.tanh(x)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the vanishing gradient problem?", "options": {"A": "A problem where gradients become too small for effective learning in deep networks", "B": "A problem where gradients become too large for effective learning in deep networks", "C": "A problem where gradients oscillate and do not converge", "D": "A problem where gradients diverge to infinity"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "How do you implement the mean squared error (MSE) loss function in Python?", "options": {"A": "def mse(y_true, y_pred): return np.mean((y_true - y_pred)**2)", "B": "def mse(y_true, y_pred): return np.sum((y_true - y_pred)**2)", "C": "def mse(y_true, y_pred): return np.abs(y_true - y_pred)", "D": "def mse(y_true, y_pred): return np.mean(np.abs(y_true - y_pred))"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the role of a loss function in training neural networks?", "options": {"A": "To measure how well the model's predictions match the actual outcomes", "B": "To sort the training data", "C": "To compress the input data", "D": "To encrypt the model weights"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is a common issue when the learning rate is set too high?", "options": {"A": "The model may diverge and fail to converge", "B": "The model may learn too quickly", "C": "The model may encrypt data", "D": "The model may compress data inefficiently"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is an example of a non-linear activation function?", "options": {"A": "ReLU", "B": "Linear", "C": "Identity", "D": "Binary Step"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of the softmax function in neural networks?", "options": {"A": "To convert logits to probabilities for multi-class classification", "B": "To sort data into different classes", "C": "To compress output vectors", "D": "To encrypt class labels"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the primary difference between batch gradient descent and stochastic gradient descent (SGD)?", "options": {"A": "Batch gradient descent uses the entire dataset to compute gradients, while SGD uses one sample at a time", "B": "Batch gradient descent sorts data, while SGD encrypts data", "C": "Batch gradient descent compresses data, while SGD scales data", "D": "Batch gradient descent is faster than SGD"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is the purpose of dropout in training neural networks?", "options": {"A": "To prevent overfitting by randomly dropping units during training", "B": "To sort data", "C": "To encrypt data", "D": "To compress data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How does the learning rate affect the training of a neural network?", "options": {"A": "It determines the step size for updating weights", "B": "It sorts the data", "C": "It compresses the data", "D": "It encrypts the data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What is a hyperparameter in the context of neural networks?", "options": {"A": "A parameter whose value is set before the learning process begins", "B": "A parameter learned from the data", "C": "A type of activation function", "D": "A type of loss function"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "Which of the following is NOT a common activation function?", "options": {"A": "Softmax", "B": "Sigmoid", "C": "ReLU", "D": "Fourier"}, "correct_answers": ["D"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "Which optimization algorithm is known for its ability to adapt the learning rate of each parameter?", "options": {"A": "Adam", "B": "SGD", "C": "RMSprop", "D": "Adagrad"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is a common technique to mitigate the vanishing gradient problem?", "options": {"A": "Using ReLU activation functions", "B": "Using linear activation functions", "C": "Decreasing the learning rate", "D": "Using larger batch sizes"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "easy", "type": "multiple_choice", "question": "What does the term 'epoch' refer to in training neural networks?", "options": {"A": "One complete pass through the entire training dataset", "B": "One update of weights", "C": "One mini-batch of data", "D": "One gradient calculation"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the purpose of a validation set in training neural networks?", "options": {"A": "To evaluate the model's performance on unseen data during training", "B": "To sort the training data", "C": "To encrypt the model weights", "D": "To compress the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the primary goal of early stopping in neural network training?", "options": {"A": "To prevent overfitting by stopping training when performance on a validation set decreases", "B": "To increase training speed", "C": "To encrypt the model weights", "D": "To compress the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "Which type of neural network is most suitable for processing sequential data?", "options": {"A": "Recurrent Neural Network (RNN)", "B": "Convolutional Neural Network (CNN)", "C": "Fully Connected Network (FCN)", "D": "Radial Basis Function Network (RBFN)"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "What is the purpose of using a learning rate scheduler during training?", "options": {"A": "To adjust the learning rate dynamically based on the training process", "B": "To sort the training data", "C": "To encrypt the model weights", "D": "To compress the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "How does batch normalization help in training deep neural networks?", "options": {"A": "By normalizing the inputs of each layer, it reduces internal covariate shift", "B": "By sorting the training data", "C": "By encrypting the model weights", "D": "By compressing the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "hard", "type": "multiple_choice", "question": "Which of the following is a common method to initialize the weights of a neural network?", "options": {"A": "Xavier Initialization", "B": "Binary Initialization", "C": "Symmetric Initialization", "D": "Zero Initialization"}, "correct_answers": ["A"], "is_multiple_choice": false}
{"skill": "Neural Networks", "difficulty": "medium", "type": "multiple_choice", "question": "What is the main advantage of using convolutional layers in neural networks?", "options": {"A": "They are able to capture spatial hierarchies in data", "B": "They sort the input data", "C": "They encrypt the model weights", "D": "They compress the input data"}, "correct_answers": ["A"], "is_multiple_choice": false}